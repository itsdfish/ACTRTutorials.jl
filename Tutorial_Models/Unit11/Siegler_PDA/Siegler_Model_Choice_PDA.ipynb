{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this tutorial is to develop an approximate likelihood function for the Siegler addition model using Probability Density Approximation (PDA). The basic idea underlying PDA is that the likelihood can be approximated by simulating the data-generating process many times. Although this can be computationally intensive, it works even when an analytic likelihood function is difficult or unknown. \n",
    "\n",
    "One of the important lessons in this tutorial is how to speed up PDA. A speed up can be accomplished with at least two techniques. One technique involves caching results to eliminate redundant computations. For example, if a set of responses are identical and independently distributed under the model, a set of simulated responses can be cached and reused to approximate the likelihood function. Second, independent components of the model can be simulated on seperate processors. In many situations, there are several ways to delegate work to multiple processors. To achieve maximum efficiency, it is important to (1) keep all processors occupied as much as possible, and (2) minimize the proportion of time attributable to overhead. Running a simulation on parallel processors requires some overhead to manage the jobs and to transfer data to and from the processors. A simulation will run relatively fast if all processors are busy and overhead is small relative to the processing time of the simulation. \n",
    "\n",
    "# Siegler Addition Task\n",
    "\n",
    "The Siegler addition task is based on a study of childrens' arithmatic ability conducted by Siegler (1984). In the task, children are asked to sum two verbally presented numbers (e.g. 2 + 2), and provide the solution verbally. If the participant does not know the solution, he or she responds \"I don't know\".  Each block consists of the following problems:\n",
    "\n",
    "- 1 + 1\n",
    "- 1 + 2\n",
    "- 1 + 3\n",
    "- 2 + 2\n",
    "- 2 + 3 \n",
    "- 3 + 3\n",
    "\n",
    "The block is repeated five times in the present simulation.\n",
    "\n",
    "\n",
    "# Siegler Addition Model\n",
    "\n",
    "## Overview\n",
    "\n",
    "On each trial, the Siegler addition model proceeds through a deterministic chain of production rules that build the problem representation in the imaginal buffer, retrieve the answer, and respond. For each number, the model fires a production rule that listens to the number, retrieves the number from declarative memory, and adds the number to a problem representation chunk in the imaginal buffer. The resulting chunk is used as a retrieval request in the next production rule to obtain the answer from declarative memory. When the next production rule fires, it \"harvests\" the answer, which entails assigning the answer to a sum slot in the problem representation chunk. Finally, the model fires a production rule to vocalize the answer and merge the problem representation into declarative memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarative memory\n",
    "\n",
    "Declarative memory $M$ is populated with 35 chunks representing addition facts. As an example, consider the chunk $m$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{c}_m = \\{\\rm (addend1,2),(addend2,2), (sum,4)\\}\n",
    "\\end{align}\n",
    "\n",
    "Each chunk contains the following three slots: $Q = \\{\\textrm{addend1},\\textrm{addend2}, \\textrm{sum}\\}$ .The full set of 35 addition facts is generated by permuting integers ranging from 0 to 5 for addend1 and addend2, such that the sum is less 10. Formally, this set is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "M = \\{ \\forall_{i,j} \\rm [(addend1, j), (addend2, i), (sum, i + j)] : i,j \\in \\{0,1,\\dots 5\\}, i + j < 10  \\}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Request\n",
    "\n",
    "The details of the problem are encoded through the aural module where they are transferred and stored in a chunk $\\mathbf{c}_s$ located in the imaginal buffer. A retrieval request on trial $i$ is formed from the slots in $\\mathbf{c}_s$, which is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{r}_i = \\{(\\rm addend1, c_s(addend1), (\\rm addend2, c_s(addend2) \\}\n",
    "\\end{align}\n",
    "\n",
    "where $Q_r = \\{\\rm addend1, addend2\\}$ is the set of slots for $\\mathbf{r}_i$. \n",
    "\n",
    "## Activation\n",
    "\n",
    "Activation for chunk $\\mathbf{c}_m$ is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "a_m = \\textrm{blc}_m + \\rho_m + \\epsilon_m\n",
    "\\end{align}\n",
    "\n",
    "where $\\textrm{blc}_m$ represents degree of prior practice, $\\rho_m$ is partial matching activation and $\\epsilon \\sim \\rm normal(0,s)$. Unlike previous models, blc is indexed by $m$ to reflect the fact that its value depends on the chunk. The rationale for allowing blc to vary as function of $m$ is due to the fact that children have more practice with addition problems in which the sum is less than five, and thus, perform better with these problems. Based on this fact, we can define blc with the following piecewise equation:\n",
    "\n",
    "\\begin{align}\\label{eq:penalty_activation_siegler}\n",
    "\\textrm{blc}_m  = \\begin{cases}\n",
    "    .65 & \\text{if } c_m(\\rm sum) < 5 \\\\\n",
    "    0 & \\text{otherwise} \\\\\n",
    "  \\end{cases}\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The model uses the following graded mismatch penalty function\n",
    "\n",
    "\\begin{align}\n",
    "\\rho_m  = -\\textrm{sim} \\times \\delta \\sum_{q \\in Q_r}| c_m(q) - r(q) | \n",
    "\\end{align}\n",
    "\n",
    "where sim = .1 is a similarity scalar. The penalty function applies greater penalty when the slot values differ by a larger degree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Response Mapping\n",
    "\n",
    "Due to the partial matching mechanism, a given response could be associated with one of several possible chunks. For example, the response $y = 4$ could have resulted from retrieving any chunk that represents the following addition facts:\n",
    "- $2+2$\n",
    "- $1+3$\n",
    "- $3+1$ \n",
    "- $4+0$\n",
    "- $0+4$\n",
    "\n",
    "In order to account for this in the likelihood function, we define the set $R_i$, which consists of all possible chunks associated with response $y_i$ on trial $i$. Formally, this is defined as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "R_i = \\{\\mathbf{c}_m \\in M : c_m(\\rm sum) = \\textrm{y_i} \\} \\textrm{ for } y_i \\neq \\emptyset\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a response indicating that the sum is unknown ($y_i =\\emptyset$) when no chunk is retrieved. \n",
    "\n",
    "## Response Probability\n",
    "\n",
    "In actuality, the approximate response probability is known. To review, let $y_i \\in \\{1,2,\\dots,\\emptyset\\}$ by the response on trial $i$, where $\\emptyset$ represents a retrieval failure response. The approximate probability is given by the expression:\n",
    "\n",
    "\\begin{align}\n",
    "     \\Pr(Y_i = y_i \\mid \\delta ; \\mathbf{r}_i) \\approx \\frac{\\sum_{\\mathbf{c}_m \\in R_i} e^{\\frac{\\mu_m}{\\sigma}}}{\\sum_{\\mathbf{c}_k \\in M} e^{\\frac{\\mu_k}{\\sigma}} + e^{\\frac{\\mu_{m^\\prime}}{\\sigma}}}\n",
    "\\end{align}\n",
    "\n",
    "However, let's assume that we do not have an expression for the probability or approximate probability. In that case, we can generate data from the model to approximate the true probability. Let $X = \\{x_1,x_1,\\dots, x_{n_s}\\}$ be a set of $n_s$ simulated data. Thus, the approximate probability of observing $y_i$\n",
    "\\begin{equation}\n",
    " \\Pr(Y_i = y_i \\mid \\delta ; \\mathbf{r}_i) \\approx \\frac{1}{n_s} \\sum_{i=1}^{n_s} I(x_i, y_i)\n",
    "\\end{equation}\n",
    "where indicator function $I(x_i,h)$ yields 1 if the inputs are equal and 0 otherwise. As $n_s$ increases, the approximation becomes more accurate and converges on the analytic likelihood in the limit.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "The following is a summary of the model's assumptions:\n",
    "\n",
    "1. Retrieval failures are possible\n",
    "2. Retrieval probabilities are independent\n",
    "3. No learning occurs\n",
    "4. Activation is a decreasing function of the difference between the slot values of a chunk and and the slot values of the retrieval request\n",
    "5. Errors are due to retrieving the wrong information rather than encoding the information incorrectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processes\n",
    "\n",
    "Sizable performance gains can be achieved by running code independently on multiple processors. To run parallel code, we will load the `Distributed` package and call the function `addprocs`. If your computer does not have 4 processors, you can change the argument below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/.julia/dev/ACTRTutorial`\n",
      "┌ Info: Precompiling StatsPlots [f3b207a7-027a-5e70-b257-86293d7955fd]\n",
      "└ @ Base loading.jl:1423\n",
      "┌ Info: Precompiling MCMCChains [c7f686f2-ff18-58e9-bc7b-31028e88f75d]\n",
      "└ @ Base loading.jl:1423\n",
      "┌ Info: Precompiling ACTRModels [c095b0ea-a6ca-5cbd-afed-dbab2e976880]\n",
      "└ @ Base loading.jl:1423\n",
      "┌ Info: Precompiling DifferentialEvolutionMCMC [607db5a9-722a-4af8-9a06-1810c0fe385b]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set current directory to directory that contains this file\n",
    "cd(@__DIR__)\n",
    "using Pkg, Distributed\n",
    "# activate tutorial environment\n",
    "Pkg.activate(\"../../..\")\n",
    "# load packages\n",
    "using StatsPlots, DataFrames, MCMCChains\n",
    "using ACTRModels, Distributions, DifferentialEvolutionMCMC\n",
    "# specify the number of processors\n",
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `nprocs` to verify that the additional processors where successfully added. The return value should be 1 (main processor) plus the number of processors specified in `addprocs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @everywhere macro\n",
    "\n",
    "We will use the `@everywhere` macro to load the required code and packages to each processor. When the `@everywhere` macro prefixes a line of code, that code is loaded to each parallel worker. In cases where multiple lines of code need to be added to the processors, we can wrap the code in a `begin` block and apply `@everywhere` to the `begin` block. In the following block, we added the path to our environment, the required packages, the required code, and seed for the random number generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 4:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mThe call to compilecache failed to create a usable precompiled cache file for ACTRModels [c095b0ea-a6ca-5cbd-afed-dbab2e976880]\n",
      "      From worker 4:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  exception = Required dependency SequentialSamplingModels [0e71a2a6-2b30-4447-8742-d083a85e82d1] failed to load from a cache file.\n",
      "      From worker 4:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1132\u001b[39m\n",
      "      From worker 2:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mThe call to compilecache failed to create a usable precompiled cache file for ACTRModels [c095b0ea-a6ca-5cbd-afed-dbab2e976880]\n",
      "      From worker 2:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  exception = Invalid input in module list: expected SequentialSamplingModels.\n",
      "      From worker 2:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1132\u001b[39m\n",
      "      From worker 3:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mThe call to compilecache failed to create a usable precompiled cache file for ACTRModels [c095b0ea-a6ca-5cbd-afed-dbab2e976880]\n",
      "      From worker 3:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  exception = Invalid input in module list: expected SequentialSamplingModels.\n",
      "      From worker 3:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1132\u001b[39m\n",
      "      From worker 5:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mThe call to compilecache failed to create a usable precompiled cache file for ACTRModels [c095b0ea-a6ca-5cbd-afed-dbab2e976880]\n",
      "      From worker 5:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  exception = Required dependency SequentialSamplingModels [0e71a2a6-2b30-4447-8742-d083a85e82d1] failed to load from a cache file.\n",
      "      From worker 5:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1132\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@everywhere begin\n",
    "  # path to the tutorial environment\n",
    "  push!(LOAD_PATH, \"../../..\")\n",
    "  # required packages\n",
    "  using ACTRModels, Distributions, DifferentialEvolutionMCMC\n",
    "  # required model code\n",
    "  include(\"Siegler_Model_Choice_PDA.jl\")\n",
    "  include(\"../../../Utilities/Utilities.jl\")\n",
    "  # seed the random number generator\n",
    "  Random.seed!(774145)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pmap\n",
    "\n",
    "`pmap` is a parallel version of `map`. We will use this later to simulate the model in parallel. Recall that the function `map` simply broadcasts or applies arguments to a function. As a simple example, we will create a function `hello` that prints \"hello\" an apply it to `map` ten times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "@everywhere hello() = println(\"hello\")\n",
    "map(_->hello(), 1:10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `_->` is used when a function does not accept any arguments. `pmap` is simple and works the same way as `map`. The worker (i.e. processor) id is printed automatically along with the print statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "pmap(_->hello(), 1:10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, manual management of the jobs is not necessary. `pmap` schedules the next job as soon as a worker is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data\n",
    "\n",
    "In the code block below,  we will define a function to generate simulated data from the model. The `simulate` function accepts the following arguments:\n",
    "\n",
    "- stimuli: a `NamedTuple` of addition problems. \n",
    "- fixed_parms: a `NamedTuple` of fixed parameters\n",
    "- args...: list of keyword arguments for estimated parameters\n",
    "\n",
    "In the annotated code below, `simulate` calls two other functions: `initialize_model` and `simulate_trial`. The function `initialize_model` returns a new ACT-R model object based on `fixed_parms` and other paramenters specified in `args...`. The function `simulate_trial` generates a response for a given stimulus. The reason for creating a separate function for simulating a single trial will become appearent later when we need to generate many samples for a given stimulus in order to approximate the likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simulate_trail (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function simulate(stimuli, fixed_parms; args...)\n",
    "    N = length(stimuli)\n",
    "    # initialize data\n",
    "    data = Array{NamedTuple,1}(undef, N)\n",
    "    # initialize ACT-R model \n",
    "    actr = initialize_model(fixed_parms; args...)\n",
    "    # simulate each trial with stimulus s\n",
    "    for (i,s) in enumerate(stimuli)\n",
    "        response = simulate_trail(actr, s)\n",
    "        data[i] = (s..., resp=response)\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "function initialize_model(fixed_parms; args...)\n",
    "   # populate memory with addition facts\n",
    "   chunks = populate_memory()\n",
    "   # set blc parameters for each chunk\n",
    "   set_baselevels!(chunks)\n",
    "   # add parameters and chunks to declarative memory\n",
    "   memory = Declarative(;memory=chunks)\n",
    "   # add declarative memory to ACTR object\n",
    "   return actr = ACTR(;declarative=memory, fixed_parms..., args...)\n",
    "end\n",
    "\n",
    "function simulate_trail(actr, stimulus)\n",
    "    # retrieve chunk using stimulus as retrieval request\n",
    "    chunk = retrieve(actr; stimulus...)\n",
    "    # return sum slot if retrieved, -100 for retrieval failure\n",
    "    return isempty(chunk) ? -100 : chunk[1].slots.sum\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below generates 5 blocks of data using parameters $\\delta = 16$. The data for each trial is a `NamedTuple` consisting of \n",
    "\n",
    "- num1: the first addend of the problem\n",
    "- num2: the second addedn of the problem\n",
    "- resp: the response where -100 indicates \"I don't know\"\n",
    "- N: the number of instances of a specific problem-response combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: sim_fun not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: sim_fun not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[7]:3",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "n_blocks = 5\n",
    "parms = (δ=16.0,)\n",
    "fixed_parms = (s=.5, τ=-.45, mmp = true,noise = true,mmp_fun = sim_fun,ter = 2.05)\n",
    "stimuli = [(num1 = 1,num2 = 1), (num1 = 1,num2 = 2), (num1 = 1,num2 = 3), (num1 = 2,num2 = 2),\n",
    "    (num1 = 2,num2 = 3), (num1 = 3,num2 = 3)]\n",
    "temp = mapreduce(_->simulate(stimuli, fixed_parms; parms...), vcat, 1:n_blocks)\n",
    "temp = unique_data(temp)\n",
    "sort!(temp)\n",
    "data = map(x->filter(y->y.num1==x.num1 && y.num2==x.num2 , temp), stimuli);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most time consuming part of approximating the likelihood function is generating simulated data. Any measure to cache and reuse simulated data will improve performance. In the code block above, two measures were taken to improve efficiency. First, we used `unique` to count the number of unique responses. For example, there were four responses of $2$ to $1 + 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: data not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: data not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[8]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "data[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such cases, we can multiply the approximate log probability of responding $2$ to the question $1 + 1$ by 4. The second effeciency measure involves grouping data according to stimuli so that the simulated data can be reused, even if the responses are different. Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: data not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: data not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[9]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sub-vector contains all responses to the question $1 + 1$, which is composed of 4 responses of 2 and 1 response of 3. By grouping the data according to stimuli, we can use the same simulated data when approximating the response probabilities. In this example, we will first determine the relative frequency of $2$ and mutiply the log probability by 4, and then determine the relative frequency of $3$ using the same simulated data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Likelihood Function\n",
    "\n",
    "The function `loglike` is the primary function responsible for computing the log likelihood of data and is passed to the MCMC sampler. `loglike` requires the following inputs:\n",
    "\n",
    "- data: a vector of vectors of `NamedTuples` grouped by stimulus representing the data\n",
    "- stimuli: a vector of stimuli \n",
    "- fixed_parms: a `NamedTuple` of fixed parameters and settings\n",
    "- $\\delta$: mismatch penalty parameter\n",
    "- n_sim: a keyword argument for the number of simulations for approximating the likelihood function\n",
    "\n",
    "`loglike` performs the following operations: first, an ACT-R model object is created with the parameters in `fixed_parms` and $\\delta$. Next, a temporary function is created which accepts a data vector and a corresponding stimulus. The function `pmap` broadcasts each pair in `data` and `stimuli` to a separate processor, where the log likelihood is computed in `loglike_trial`.  Finally, the sum of all log likelihoods is returned.\n",
    "\n",
    "The function `loglike_trial` computes the approximate log likelihood for each trail. Techically, `loglike_trial` computes the log likelihood for trials with the same stimulus. It requires the following inputs:\n",
    "\n",
    "- actr: an ACT-R model object\n",
    "- stimuli: a vector of stimuli \n",
    "- fixed_parms: a `NamedTuple` of fixed parameters and settings\n",
    "- data: a vector of `NamedTuples` of data for a given stimulus\n",
    "- n_sim: the number of simulations for approximating the likelihood function\n",
    "\n",
    "`loglike_trial` first generates `n_sim` responses for a given stimulus. In the for loop, the approximate log probability is computed from the simulated responses. The log likelihood of each response in $d$ is weighted by the number of responses of the same type. For example, if there were 3 responses of 2 to the question $1+1$, the log likelihood will be weighted by 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loglike_trial (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loglike(data, stimuli, fixed_parms, δ; n_sim=1000)\n",
    "    # initialize the model with fixed_parms and δ\n",
    "    actr = initialize_model(fixed_parms; δ)\n",
    "    # temporary function for loglike_trial with two arguments\n",
    "    f(s, x) = loglike_trial(actr, s, x, n_sim)\n",
    "    # map the corresponding elements of stimuli and data to each processor\n",
    "    LLs = pmap(f, stimuli, data)\n",
    "    # sum the log likelihood. (Annotate return type because pmap is not type-stable)\n",
    "    return sum(LLs)::Float64\n",
    "end\n",
    "\n",
    "function loglike_trial(actr, stimulus, data, n_sim) \n",
    "    # simulate trial n_sim times \n",
    "    preds = map(_->simulate_trail(actr, stimulus), 1:n_sim)\n",
    "    LL = 0.0\n",
    "    # compute approximate log likelihood for each trial in data \n",
    "    for d in data \n",
    "        p = max(mean(preds .== d.resp), 1/n_sim)\n",
    "        # multiply log likelihood by number of replicates N\n",
    "        LL += log(p)*d.N\n",
    "    end\n",
    "    return LL\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "The following summaries the prior distributions and the likelihood. \n",
    "\n",
    "\\begin{align}\n",
    "\\delta \\sim \\textrm{Normal}(16,8) \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " \\theta_i  \\approx \\frac{1}{n_s} \\sum_{i=1}^{n_s} I(x_i, y_i)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "y_i \\sim \\textrm{Bernoulli}(\\theta_i)\n",
    "\\end{align}\n",
    "\n",
    "Next, we will set up the model and MCMC sampler. First, we specify the prior distributions and the boundaries for each parameter. Next, a model object is defined. The optional positional objects are passed to `loglike`. The keyword arguments correspond to the prior distribution, the `loglike` function of the model, and the `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: Normal not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Normal not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[11]:2",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# prior distribution over δ\n",
    "priors = (\n",
    "    δ = (Normal(16, 8),),\n",
    ")\n",
    "\n",
    "# boundaries for δ\n",
    "bounds = ((-Inf,Inf),)\n",
    "\n",
    "# model object.\n",
    "model = DEModel(stimuli, fixed_parms; priors, model=loglike, data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Parameters\n",
    "\n",
    "We will estimate the parameter $\\delta$ using Differential Evolution MCMC because it can be used with approximate likelihood functions. We will run one group of eight particles for 2,000 iterations and discard the first 1,000 warmup samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: bounds not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: bounds not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[12]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "de = DE(;bounds, burnin=1000, priors, n_groups=1, Np=8)\n",
    "n_iter = 2000\n",
    "chain = sample(model, de, n_iter, progress=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Diagnostics\n",
    "\n",
    "The first panel for each plot shows acceptable mixing between the eight chains. Furthermore, $\\hat{r} \\approx 1$ for $\\delta$, indicating that the chains converged.  In the second panel for each plot, the auto-correlation between successive samples drops close to zero after a lag of about 10-15, which is typical for Differential Evolution MCMC, particularly with approximation error in the likelihood function. \n",
    "### Posterior Distributions \n",
    "\n",
    "The posterior distributions in the third panel of each plot encompass the data generating value ($\\delta = 16$), indicating acceptable parameter recovery. Although the posterior distributions are somewhat wide, their standard deviations are smaller compared to the standard deviations of the prior distributions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: pyplot not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: pyplot not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[13]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "pyplot()\n",
    "font_size = 12\n",
    "ch = group(chain, :δ)\n",
    "p1 = plot(ch, xaxis=font(font_size), yaxis=font(font_size), seriestype=(:traceplot),\n",
    "  grid=false, size=(250,100), titlefont=font(font_size))\n",
    "p2 = plot(ch, xaxis=font(font_size), yaxis=font(font_size), seriestype=(:autocorplot),\n",
    "  grid=false, size=(250,100), titlefont=font(font_size))\n",
    "p3 = plot(ch, xaxis=font(font_size), yaxis=font(font_size), seriestype=(:mixeddensity),\n",
    "  grid=false, size=(250,100), titlefont=font(font_size))\n",
    "pcτ = plot(p1, p2, p3, layout=(3,1), size=(800,600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooled density plot provides a smoother depiction of the posterior distribution of $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: font not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: font not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[14]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "pooleddensity(ch, grid=false, xaxis=font(font_size), yaxis=font(font_size), size=(800,250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Distribution\n",
    "\n",
    "The code block below generates the posterior distribution of of response probabilities for each of the six problems. A similar pattern emerges across all six problems: the correct answer is the most likely response at $\\approx .6$ and the probability of a response becomes less and less likely as it deviates further from the correct response. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: posterior_predictive not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: posterior_predictive not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[15]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "preds = posterior_predictive(x -> simulate(stimuli, parms; x...), chain, 1000)\n",
    "preds = vcat(vcat(preds...)...)\n",
    "df = DataFrame(preds)\n",
    "p5 = response_histogram(df, stimuli; size=(800,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Siegler, R. S., & Shrager, J. (1984). Strategy choices in addition and\n",
    "subtraction: How do children know what to do? In C. Sophian (Ed.),\n",
    "The origins of cognitive skills (pp. 229-293). Hillsdale, NJ: Erlbaum\n",
    "\n",
    "Turner, B. M., & Van Zandt, T. (2012). A tutorial on approximate Bayesian computation. Journal of Mathematical Psychology, 56(2), 69-85.\n",
    "\n",
    "Turner, B. M., & Sederberg, P. B. (2014). A generalized, likelihood-free method for posterior estimation. Psychonomic bulletin & review, 21(2), 227-250.\n",
    "\n",
    "Turner, B. M., Sederberg, P. B., Brown, S. D., & Steyvers, M. (2013). A method fo"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
